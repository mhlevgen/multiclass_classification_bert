{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from transformers import BertTokenizerFast as BertTokenizer\n",
    "\n",
    "from bert_settings import BATCH_SIZE, N_EPOCHS, MAX_TOKEN_COUNT, BERT_MODEL_NAME\n",
    "from bert_utils import ClassifierModel, DataModule\n",
    "\n",
    "from metrics import confusion_matrix_plot, calculate_recall, calculate_roc_auc, calculate_precision\n",
    "from preprocessing import process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 42\n",
    "pl.seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/testset_C.csv', sep=';')\n",
    "data = process_data(data)\n",
    "\n",
    "train_data, test_data = train_test_split(data, stratify=data['productgroup'], test_size=0.2, random_state=1)\n",
    "\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "train_y = enc.fit_transform(train_data[['productgroup']])\n",
    "test_y = enc.transform(test_data[['productgroup']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = len(train_data) // BATCH_SIZE\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "model = ClassifierModel(\n",
    "    n_classes=4,\n",
    "    n_warmup_steps=10,\n",
    "    n_training_steps=total_training_steps\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "data_module = DataModule(\n",
    "    train_data,\n",
    "    test_data,\n",
    "    tokenizer,\n",
    "    train_y,\n",
    "    test_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_token_len=MAX_TOKEN_COUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"bert-model-v2-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"text-classification\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[early_stopping_callback, checkpoint_callback],\n",
    "    max_epochs=N_EPOCHS,\n",
    "    accelerator='gpu',\n",
    "    devices=[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:616: UserWarning: Checkpoint directory /home/jovyan/work/nemo_s2t/notebooks_old/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type             | Params\n",
      "------------------------------------------------\n",
      "0 | bert       | BertModel        | 109 M \n",
      "1 | classifier | Linear           | 3.1 K \n",
      "2 | criterion  | CrossEntropyLoss | 0     \n",
      "------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.941   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  80%|███████▉  | 214/268 [02:48<00:42,  1.27it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/54 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/54 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:  80%|████████  | 215/268 [02:49<00:41,  1.27it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  81%|████████  | 216/268 [02:49<00:40,  1.28it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  81%|████████  | 217/268 [02:49<00:39,  1.28it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  81%|████████▏ | 218/268 [02:49<00:38,  1.29it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  82%|████████▏ | 219/268 [02:49<00:38,  1.29it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  82%|████████▏ | 220/268 [02:50<00:37,  1.29it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  82%|████████▏ | 221/268 [02:50<00:36,  1.30it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  83%|████████▎ | 222/268 [02:50<00:35,  1.30it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  83%|████████▎ | 223/268 [02:50<00:34,  1.31it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  84%|████████▎ | 224/268 [02:50<00:33,  1.31it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  84%|████████▍ | 225/268 [02:51<00:32,  1.31it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  84%|████████▍ | 226/268 [02:51<00:31,  1.32it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  85%|████████▍ | 227/268 [02:51<00:31,  1.32it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  85%|████████▌ | 228/268 [02:51<00:30,  1.33it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  85%|████████▌ | 229/268 [02:52<00:29,  1.33it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  86%|████████▌ | 230/268 [02:52<00:28,  1.33it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  86%|████████▌ | 231/268 [02:52<00:27,  1.34it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  87%|████████▋ | 232/268 [02:52<00:26,  1.34it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  87%|████████▋ | 233/268 [02:53<00:25,  1.35it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  87%|████████▋ | 234/268 [02:53<00:25,  1.35it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  88%|████████▊ | 235/268 [02:53<00:24,  1.35it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  88%|████████▊ | 236/268 [02:53<00:23,  1.36it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  88%|████████▊ | 237/268 [02:53<00:22,  1.36it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  89%|████████▉ | 238/268 [02:54<00:21,  1.37it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  89%|████████▉ | 239/268 [02:54<00:21,  1.37it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  90%|████████▉ | 240/268 [02:54<00:20,  1.37it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  90%|████████▉ | 241/268 [02:54<00:19,  1.38it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  90%|█████████ | 242/268 [02:55<00:18,  1.38it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  91%|█████████ | 243/268 [02:55<00:18,  1.39it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  91%|█████████ | 244/268 [02:55<00:17,  1.39it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  91%|█████████▏| 245/268 [02:55<00:16,  1.39it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  92%|█████████▏| 246/268 [02:56<00:15,  1.40it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  92%|█████████▏| 247/268 [02:56<00:14,  1.40it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  93%|█████████▎| 248/268 [02:56<00:14,  1.41it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  93%|█████████▎| 249/268 [02:56<00:13,  1.41it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  93%|█████████▎| 250/268 [02:56<00:12,  1.41it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  94%|█████████▎| 251/268 [02:57<00:11,  1.42it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  94%|█████████▍| 252/268 [02:57<00:11,  1.42it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  94%|█████████▍| 253/268 [02:57<00:10,  1.42it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  95%|█████████▍| 254/268 [02:57<00:09,  1.43it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  95%|█████████▌| 255/268 [02:58<00:09,  1.43it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  96%|█████████▌| 256/268 [02:58<00:08,  1.44it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  96%|█████████▌| 257/268 [02:58<00:07,  1.44it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  96%|█████████▋| 258/268 [02:58<00:06,  1.44it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  97%|█████████▋| 259/268 [02:58<00:06,  1.45it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  97%|█████████▋| 260/268 [02:59<00:05,  1.45it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  97%|█████████▋| 261/268 [02:59<00:04,  1.45it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  98%|█████████▊| 262/268 [02:59<00:04,  1.46it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  98%|█████████▊| 263/268 [02:59<00:03,  1.46it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  99%|█████████▊| 264/268 [03:00<00:02,  1.47it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  99%|█████████▉| 265/268 [03:00<00:02,  1.47it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0:  99%|█████████▉| 266/268 [03:00<00:01,  1.47it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0: 100%|█████████▉| 267/268 [03:00<00:00,  1.48it/s, loss=0.0286, v_num=3, train_loss=0.0128]\n",
      "Epoch 0: 100%|██████████| 268/268 [03:01<00:00,  1.48it/s, loss=0.0286, v_num=3, train_loss=0.0128, val_loss=0.0347]\n",
      "Epoch 0: 100%|██████████| 268/268 [03:01<00:00,  1.48it/s, loss=0.0286, v_num=3, train_loss=0.0128, val_loss=0.0347]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 214: 'val_loss' reached 0.03466 (best 0.03466), saving model to '/home/jovyan/work/nemo_s2t/notebooks_old/checkpoints/bert-model-v2-epoch=00-val_loss=0.03.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  80%|███████▉  | 214/268 [02:29<00:37,  1.43it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/54 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/54 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:  80%|████████  | 215/268 [02:30<00:37,  1.43it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  81%|████████  | 216/268 [02:30<00:36,  1.44it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  81%|████████  | 217/268 [02:30<00:35,  1.44it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  81%|████████▏ | 218/268 [02:30<00:34,  1.45it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  82%|████████▏ | 219/268 [02:30<00:33,  1.45it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  82%|████████▏ | 220/268 [02:30<00:32,  1.46it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  82%|████████▏ | 221/268 [02:31<00:32,  1.46it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  83%|████████▎ | 222/268 [02:31<00:31,  1.47it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  83%|████████▎ | 223/268 [02:31<00:30,  1.47it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  84%|████████▎ | 224/268 [02:31<00:29,  1.47it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  84%|████████▍ | 225/268 [02:32<00:29,  1.48it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  84%|████████▍ | 226/268 [02:32<00:28,  1.48it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  85%|████████▍ | 227/268 [02:32<00:27,  1.49it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  85%|████████▌ | 228/268 [02:32<00:26,  1.49it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  85%|████████▌ | 229/268 [02:33<00:26,  1.50it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  86%|████████▌ | 230/268 [02:33<00:25,  1.50it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  86%|████████▌ | 231/268 [02:33<00:24,  1.50it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  87%|████████▋ | 232/268 [02:33<00:23,  1.51it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  87%|████████▋ | 233/268 [02:33<00:23,  1.51it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  87%|████████▋ | 234/268 [02:34<00:22,  1.52it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  88%|████████▊ | 235/268 [02:34<00:21,  1.52it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  88%|████████▊ | 236/268 [02:34<00:20,  1.53it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  88%|████████▊ | 237/268 [02:34<00:20,  1.53it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  89%|████████▉ | 238/268 [02:35<00:19,  1.53it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  89%|████████▉ | 239/268 [02:35<00:18,  1.54it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  90%|████████▉ | 240/268 [02:35<00:18,  1.54it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  90%|████████▉ | 241/268 [02:35<00:17,  1.55it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  90%|█████████ | 242/268 [02:36<00:16,  1.55it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  91%|█████████ | 243/268 [02:36<00:16,  1.56it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  91%|█████████ | 244/268 [02:36<00:15,  1.56it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  91%|█████████▏| 245/268 [02:36<00:14,  1.56it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  92%|█████████▏| 246/268 [02:36<00:14,  1.57it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  92%|█████████▏| 247/268 [02:37<00:13,  1.57it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  93%|█████████▎| 248/268 [02:37<00:12,  1.58it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  93%|█████████▎| 249/268 [02:37<00:12,  1.58it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  93%|█████████▎| 250/268 [02:37<00:11,  1.58it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  94%|█████████▎| 251/268 [02:38<00:10,  1.59it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  94%|█████████▍| 252/268 [02:38<00:10,  1.59it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  94%|█████████▍| 253/268 [02:38<00:09,  1.60it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  95%|█████████▍| 254/268 [02:38<00:08,  1.60it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  95%|█████████▌| 255/268 [02:39<00:08,  1.60it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  96%|█████████▌| 256/268 [02:39<00:07,  1.61it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  96%|█████████▌| 257/268 [02:39<00:06,  1.61it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  96%|█████████▋| 258/268 [02:39<00:06,  1.62it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  97%|█████████▋| 259/268 [02:39<00:05,  1.62it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  97%|█████████▋| 260/268 [02:40<00:04,  1.62it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  97%|█████████▋| 261/268 [02:40<00:04,  1.63it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  98%|█████████▊| 262/268 [02:40<00:03,  1.63it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  98%|█████████▊| 263/268 [02:40<00:03,  1.64it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  99%|█████████▊| 264/268 [02:41<00:02,  1.64it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  99%|█████████▉| 265/268 [02:41<00:01,  1.64it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1:  99%|█████████▉| 266/268 [02:41<00:01,  1.65it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1: 100%|█████████▉| 267/268 [02:41<00:00,  1.65it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0347]\n",
      "Epoch 1: 100%|██████████| 268/268 [02:42<00:00,  1.65it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0179]\n",
      "Epoch 1: 100%|██████████| 268/268 [02:42<00:00,  1.65it/s, loss=0.0102, v_num=3, train_loss=0.00368, val_loss=0.0179]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 428: 'val_loss' reached 0.01794 (best 0.01794), saving model to '/home/jovyan/work/nemo_s2t/notebooks_old/checkpoints/bert-model-v2-epoch=01-val_loss=0.02.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  80%|███████▉  | 214/268 [02:29<00:37,  1.43it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/54 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/54 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 2:  80%|████████  | 215/268 [02:30<00:37,  1.43it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  81%|████████  | 216/268 [02:30<00:36,  1.44it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  81%|████████  | 217/268 [02:30<00:35,  1.44it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  81%|████████▏ | 218/268 [02:30<00:34,  1.45it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  82%|████████▏ | 219/268 [02:31<00:33,  1.45it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  82%|████████▏ | 220/268 [02:31<00:32,  1.45it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  82%|████████▏ | 221/268 [02:31<00:32,  1.46it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  83%|████████▎ | 222/268 [02:31<00:31,  1.46it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  83%|████████▎ | 223/268 [02:31<00:30,  1.47it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  84%|████████▎ | 224/268 [02:32<00:29,  1.47it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  84%|████████▍ | 225/268 [02:32<00:29,  1.48it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  84%|████████▍ | 226/268 [02:32<00:28,  1.48it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  85%|████████▍ | 227/268 [02:32<00:27,  1.49it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  85%|████████▌ | 228/268 [02:33<00:26,  1.49it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  85%|████████▌ | 229/268 [02:33<00:26,  1.49it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  86%|████████▌ | 230/268 [02:33<00:25,  1.50it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  86%|████████▌ | 231/268 [02:33<00:24,  1.50it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  87%|████████▋ | 232/268 [02:33<00:23,  1.51it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  87%|████████▋ | 233/268 [02:34<00:23,  1.51it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  87%|████████▋ | 234/268 [02:34<00:22,  1.52it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  88%|████████▊ | 235/268 [02:34<00:21,  1.52it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  88%|████████▊ | 236/268 [02:34<00:21,  1.52it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  88%|████████▊ | 237/268 [02:35<00:20,  1.53it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  89%|████████▉ | 238/268 [02:35<00:19,  1.53it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  89%|████████▉ | 239/268 [02:35<00:18,  1.54it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  90%|████████▉ | 240/268 [02:35<00:18,  1.54it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  90%|████████▉ | 241/268 [02:36<00:17,  1.54it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  90%|█████████ | 242/268 [02:36<00:16,  1.55it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  91%|█████████ | 243/268 [02:36<00:16,  1.55it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  91%|█████████ | 244/268 [02:36<00:15,  1.56it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  91%|█████████▏| 245/268 [02:36<00:14,  1.56it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  92%|█████████▏| 246/268 [02:37<00:14,  1.57it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  92%|█████████▏| 247/268 [02:37<00:13,  1.57it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  93%|█████████▎| 248/268 [02:37<00:12,  1.57it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  93%|█████████▎| 249/268 [02:37<00:12,  1.58it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  93%|█████████▎| 250/268 [02:38<00:11,  1.58it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  94%|█████████▎| 251/268 [02:38<00:10,  1.59it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  94%|█████████▍| 252/268 [02:38<00:10,  1.59it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  94%|█████████▍| 253/268 [02:38<00:09,  1.59it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  95%|█████████▍| 254/268 [02:39<00:08,  1.60it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  95%|█████████▌| 255/268 [02:39<00:08,  1.60it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  96%|█████████▌| 256/268 [02:39<00:07,  1.61it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  96%|█████████▌| 257/268 [02:39<00:06,  1.61it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  96%|█████████▋| 258/268 [02:39<00:06,  1.61it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  97%|█████████▋| 259/268 [02:40<00:05,  1.62it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  97%|█████████▋| 260/268 [02:40<00:04,  1.62it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  97%|█████████▋| 261/268 [02:40<00:04,  1.63it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  98%|█████████▊| 262/268 [02:40<00:03,  1.63it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  98%|█████████▊| 263/268 [02:41<00:03,  1.63it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  99%|█████████▊| 264/268 [02:41<00:02,  1.64it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  99%|█████████▉| 265/268 [02:41<00:01,  1.64it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2:  99%|█████████▉| 266/268 [02:41<00:01,  1.64it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2: 100%|█████████▉| 267/268 [02:41<00:00,  1.65it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2: 100%|██████████| 268/268 [02:42<00:00,  1.65it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]\n",
      "Epoch 2: 100%|██████████| 268/268 [02:42<00:00,  1.65it/s, loss=0.0076, v_num=3, train_loss=0.00266, val_loss=0.0179]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 642: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  80%|███████▉  | 214/268 [02:28<00:37,  1.44it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/54 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/54 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 3:  80%|████████  | 215/268 [02:29<00:36,  1.44it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  81%|████████  | 216/268 [02:29<00:36,  1.44it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  81%|████████  | 217/268 [02:29<00:35,  1.45it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  81%|████████▏ | 218/268 [02:30<00:34,  1.45it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  82%|████████▏ | 219/268 [02:30<00:33,  1.46it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  82%|████████▏ | 220/268 [02:30<00:32,  1.46it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  82%|████████▏ | 221/268 [02:30<00:32,  1.47it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  83%|████████▎ | 222/268 [02:30<00:31,  1.47it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  83%|████████▎ | 223/268 [02:31<00:30,  1.47it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  84%|████████▎ | 224/268 [02:31<00:29,  1.48it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  84%|████████▍ | 225/268 [02:31<00:28,  1.48it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  84%|████████▍ | 226/268 [02:31<00:28,  1.49it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  85%|████████▍ | 227/268 [02:32<00:27,  1.49it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  85%|████████▌ | 228/268 [02:32<00:26,  1.50it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  85%|████████▌ | 229/268 [02:32<00:25,  1.50it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  86%|████████▌ | 230/268 [02:32<00:25,  1.51it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  86%|████████▌ | 231/268 [02:33<00:24,  1.51it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  87%|████████▋ | 232/268 [02:33<00:23,  1.51it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  87%|████████▋ | 233/268 [02:33<00:23,  1.52it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  87%|████████▋ | 234/268 [02:33<00:22,  1.52it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  88%|████████▊ | 235/268 [02:33<00:21,  1.53it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  88%|████████▊ | 236/268 [02:34<00:20,  1.53it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  88%|████████▊ | 237/268 [02:34<00:20,  1.53it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  89%|████████▉ | 238/268 [02:34<00:19,  1.54it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  89%|████████▉ | 239/268 [02:34<00:18,  1.54it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  90%|████████▉ | 240/268 [02:35<00:18,  1.55it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  90%|████████▉ | 241/268 [02:35<00:17,  1.55it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  90%|█████████ | 242/268 [02:35<00:16,  1.56it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  91%|█████████ | 243/268 [02:35<00:16,  1.56it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  91%|█████████ | 244/268 [02:36<00:15,  1.56it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  91%|█████████▏| 245/268 [02:36<00:14,  1.57it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  92%|█████████▏| 246/268 [02:36<00:13,  1.57it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  92%|█████████▏| 247/268 [02:36<00:13,  1.58it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  93%|█████████▎| 248/268 [02:36<00:12,  1.58it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  93%|█████████▎| 249/268 [02:37<00:11,  1.58it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  93%|█████████▎| 250/268 [02:37<00:11,  1.59it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  94%|█████████▎| 251/268 [02:37<00:10,  1.59it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  94%|█████████▍| 252/268 [02:37<00:10,  1.60it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  94%|█████████▍| 253/268 [02:38<00:09,  1.60it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  95%|█████████▍| 254/268 [02:38<00:08,  1.60it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  95%|█████████▌| 255/268 [02:38<00:08,  1.61it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  96%|█████████▌| 256/268 [02:38<00:07,  1.61it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  96%|█████████▌| 257/268 [02:38<00:06,  1.62it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  96%|█████████▋| 258/268 [02:39<00:06,  1.62it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  97%|█████████▋| 259/268 [02:39<00:05,  1.62it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  97%|█████████▋| 260/268 [02:39<00:04,  1.63it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  97%|█████████▋| 261/268 [02:39<00:04,  1.63it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  98%|█████████▊| 262/268 [02:40<00:03,  1.64it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  98%|█████████▊| 263/268 [02:40<00:03,  1.64it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  99%|█████████▊| 264/268 [02:40<00:02,  1.64it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  99%|█████████▉| 265/268 [02:40<00:01,  1.65it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3:  99%|█████████▉| 266/268 [02:41<00:01,  1.65it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3: 100%|█████████▉| 267/268 [02:41<00:00,  1.66it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0179]\n",
      "Epoch 3: 100%|██████████| 268/268 [02:41<00:00,  1.66it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0174]\n",
      "Epoch 3: 100%|██████████| 268/268 [02:42<00:00,  1.65it/s, loss=0.00168, v_num=3, train_loss=0.00152, val_loss=0.0174]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 856: 'val_loss' reached 0.01743 (best 0.01743), saving model to '/home/jovyan/work/nemo_s2t/notebooks_old/checkpoints/bert-model-v2-epoch=03-val_loss=0.02.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  80%|███████▉  | 214/268 [02:30<00:37,  1.42it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/54 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/54 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 4:  80%|████████  | 215/268 [02:30<00:37,  1.42it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  81%|████████  | 216/268 [02:30<00:36,  1.43it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  81%|████████  | 217/268 [02:31<00:35,  1.44it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  81%|████████▏ | 218/268 [02:31<00:34,  1.44it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  82%|████████▏ | 219/268 [02:31<00:33,  1.44it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  82%|████████▏ | 220/268 [02:31<00:33,  1.45it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  82%|████████▏ | 221/268 [02:32<00:32,  1.45it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  83%|████████▎ | 222/268 [02:32<00:31,  1.46it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  83%|████████▎ | 223/268 [02:32<00:30,  1.46it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  84%|████████▎ | 224/268 [02:32<00:29,  1.47it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  84%|████████▍ | 225/268 [02:32<00:29,  1.47it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  84%|████████▍ | 226/268 [02:33<00:28,  1.48it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  85%|████████▍ | 227/268 [02:33<00:27,  1.48it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  85%|████████▌ | 228/268 [02:33<00:26,  1.48it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  85%|████████▌ | 229/268 [02:33<00:26,  1.49it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  86%|████████▌ | 230/268 [02:34<00:25,  1.49it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  86%|████████▌ | 231/268 [02:34<00:24,  1.50it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  87%|████████▋ | 232/268 [02:34<00:23,  1.50it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  87%|████████▋ | 233/268 [02:34<00:23,  1.51it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  87%|████████▋ | 234/268 [02:35<00:22,  1.51it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  88%|████████▊ | 235/268 [02:35<00:21,  1.51it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  88%|████████▊ | 236/268 [02:35<00:21,  1.52it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  88%|████████▊ | 237/268 [02:35<00:20,  1.52it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  89%|████████▉ | 238/268 [02:35<00:19,  1.53it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  89%|████████▉ | 239/268 [02:36<00:18,  1.53it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  90%|████████▉ | 240/268 [02:36<00:18,  1.53it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  90%|████████▉ | 241/268 [02:36<00:17,  1.54it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  90%|█████████ | 242/268 [02:36<00:16,  1.54it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  91%|█████████ | 243/268 [02:37<00:16,  1.55it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  91%|█████████ | 244/268 [02:37<00:15,  1.55it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  91%|█████████▏| 245/268 [02:37<00:14,  1.56it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  92%|█████████▏| 246/268 [02:37<00:14,  1.56it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  92%|█████████▏| 247/268 [02:37<00:13,  1.56it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  93%|█████████▎| 248/268 [02:38<00:12,  1.57it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  93%|█████████▎| 249/268 [02:38<00:12,  1.57it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  93%|█████████▎| 250/268 [02:38<00:11,  1.58it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  94%|█████████▎| 251/268 [02:38<00:10,  1.58it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  94%|█████████▍| 252/268 [02:39<00:10,  1.58it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  94%|█████████▍| 253/268 [02:39<00:09,  1.59it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  95%|█████████▍| 254/268 [02:39<00:08,  1.59it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  95%|█████████▌| 255/268 [02:39<00:08,  1.60it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  96%|█████████▌| 256/268 [02:40<00:07,  1.60it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  96%|█████████▌| 257/268 [02:40<00:06,  1.60it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  96%|█████████▋| 258/268 [02:40<00:06,  1.61it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  97%|█████████▋| 259/268 [02:40<00:05,  1.61it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  97%|█████████▋| 260/268 [02:40<00:04,  1.62it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  97%|█████████▋| 261/268 [02:41<00:04,  1.62it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  98%|█████████▊| 262/268 [02:41<00:03,  1.62it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  98%|█████████▊| 263/268 [02:41<00:03,  1.63it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  99%|█████████▊| 264/268 [02:41<00:02,  1.63it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  99%|█████████▉| 265/268 [02:42<00:01,  1.63it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4:  99%|█████████▉| 266/268 [02:42<00:01,  1.64it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4: 100%|█████████▉| 267/268 [02:42<00:00,  1.64it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0174]\n",
      "Epoch 4: 100%|██████████| 268/268 [02:43<00:00,  1.64it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0157]\n",
      "Epoch 4: 100%|██████████| 268/268 [02:43<00:00,  1.64it/s, loss=0.00156, v_num=3, train_loss=0.00118, val_loss=0.0157]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1070: 'val_loss' reached 0.01572 (best 0.01572), saving model to '/home/jovyan/work/nemo_s2t/notebooks_old/checkpoints/bert-model-v2-epoch=04-val_loss=0.02.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  80%|███████▉  | 214/268 [02:29<00:37,  1.43it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157] \n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/54 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/54 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 5:  80%|████████  | 215/268 [02:30<00:37,  1.43it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  81%|████████  | 216/268 [02:30<00:36,  1.44it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  81%|████████  | 217/268 [02:30<00:35,  1.44it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  81%|████████▏ | 218/268 [02:30<00:34,  1.44it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  82%|████████▏ | 219/268 [02:31<00:33,  1.45it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  82%|████████▏ | 220/268 [02:31<00:33,  1.45it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  82%|████████▏ | 221/268 [02:31<00:32,  1.46it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  83%|████████▎ | 222/268 [02:31<00:31,  1.46it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  83%|████████▎ | 223/268 [02:32<00:30,  1.47it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  84%|████████▎ | 224/268 [02:32<00:29,  1.47it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  84%|████████▍ | 225/268 [02:32<00:29,  1.48it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  84%|████████▍ | 226/268 [02:32<00:28,  1.48it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  85%|████████▍ | 227/268 [02:32<00:27,  1.48it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  85%|████████▌ | 228/268 [02:33<00:26,  1.49it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  85%|████████▌ | 229/268 [02:33<00:26,  1.49it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  86%|████████▌ | 230/268 [02:33<00:25,  1.50it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  86%|████████▌ | 231/268 [02:33<00:24,  1.50it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  87%|████████▋ | 232/268 [02:34<00:23,  1.51it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  87%|████████▋ | 233/268 [02:34<00:23,  1.51it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  87%|████████▋ | 234/268 [02:34<00:22,  1.51it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  88%|████████▊ | 235/268 [02:34<00:21,  1.52it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  88%|████████▊ | 236/268 [02:34<00:21,  1.52it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  88%|████████▊ | 237/268 [02:35<00:20,  1.53it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  89%|████████▉ | 238/268 [02:35<00:19,  1.53it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  89%|████████▉ | 239/268 [02:35<00:18,  1.54it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  90%|████████▉ | 240/268 [02:35<00:18,  1.54it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  90%|████████▉ | 241/268 [02:36<00:17,  1.54it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  90%|█████████ | 242/268 [02:36<00:16,  1.55it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  91%|█████████ | 243/268 [02:36<00:16,  1.55it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  91%|█████████ | 244/268 [02:36<00:15,  1.56it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  91%|█████████▏| 245/268 [02:37<00:14,  1.56it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  92%|█████████▏| 246/268 [02:37<00:14,  1.56it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  92%|█████████▏| 247/268 [02:37<00:13,  1.57it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  93%|█████████▎| 248/268 [02:37<00:12,  1.57it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  93%|█████████▎| 249/268 [02:37<00:12,  1.58it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  93%|█████████▎| 250/268 [02:38<00:11,  1.58it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  94%|█████████▎| 251/268 [02:38<00:10,  1.58it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  94%|█████████▍| 252/268 [02:38<00:10,  1.59it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  94%|█████████▍| 253/268 [02:38<00:09,  1.59it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  95%|█████████▍| 254/268 [02:39<00:08,  1.60it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  95%|█████████▌| 255/268 [02:39<00:08,  1.60it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  96%|█████████▌| 256/268 [02:39<00:07,  1.60it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  96%|█████████▌| 257/268 [02:39<00:06,  1.61it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  96%|█████████▋| 258/268 [02:40<00:06,  1.61it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  97%|█████████▋| 259/268 [02:40<00:05,  1.62it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  97%|█████████▋| 260/268 [02:40<00:04,  1.62it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  97%|█████████▋| 261/268 [02:40<00:04,  1.62it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  98%|█████████▊| 262/268 [02:40<00:03,  1.63it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  98%|█████████▊| 263/268 [02:41<00:03,  1.63it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  99%|█████████▊| 264/268 [02:41<00:02,  1.64it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  99%|█████████▉| 265/268 [02:41<00:01,  1.64it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5:  99%|█████████▉| 266/268 [02:41<00:01,  1.64it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5: 100%|█████████▉| 267/268 [02:42<00:00,  1.65it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.0157]\n",
      "Epoch 5: 100%|██████████| 268/268 [02:42<00:00,  1.65it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.023] \n",
      "Epoch 5: 100%|██████████| 268/268 [02:42<00:00,  1.64it/s, loss=0.00124, v_num=3, train_loss=0.00147, val_loss=0.023]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 1284: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:  80%|███████▉  | 214/268 [02:30<00:37,  1.42it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Validation: 0it [00:00, ?it/s]\u001B[A\n",
      "Validation:   0%|          | 0/54 [00:00<?, ?it/s]\u001B[A\n",
      "Validation DataLoader 0:   0%|          | 0/54 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 6:  80%|████████  | 215/268 [02:31<00:37,  1.42it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  81%|████████  | 216/268 [02:31<00:36,  1.43it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  81%|████████  | 217/268 [02:31<00:35,  1.43it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  81%|████████▏ | 218/268 [02:31<00:34,  1.44it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  82%|████████▏ | 219/268 [02:31<00:33,  1.44it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  82%|████████▏ | 220/268 [02:32<00:33,  1.45it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  82%|████████▏ | 221/268 [02:32<00:32,  1.45it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  83%|████████▎ | 222/268 [02:32<00:31,  1.46it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  83%|████████▎ | 223/268 [02:32<00:30,  1.46it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  84%|████████▎ | 224/268 [02:33<00:30,  1.46it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  84%|████████▍ | 225/268 [02:33<00:29,  1.47it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  84%|████████▍ | 226/268 [02:33<00:28,  1.47it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  85%|████████▍ | 227/268 [02:33<00:27,  1.48it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  85%|████████▌ | 228/268 [02:33<00:27,  1.48it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  85%|████████▌ | 229/268 [02:34<00:26,  1.49it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  86%|████████▌ | 230/268 [02:34<00:25,  1.49it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  86%|████████▌ | 231/268 [02:34<00:24,  1.49it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  87%|████████▋ | 232/268 [02:34<00:24,  1.50it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  87%|████████▋ | 233/268 [02:35<00:23,  1.50it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  87%|████████▋ | 234/268 [02:35<00:22,  1.51it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  88%|████████▊ | 235/268 [02:35<00:21,  1.51it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  88%|████████▊ | 236/268 [02:35<00:21,  1.52it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  88%|████████▊ | 237/268 [02:35<00:20,  1.52it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  89%|████████▉ | 238/268 [02:36<00:19,  1.52it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  89%|████████▉ | 239/268 [02:36<00:18,  1.53it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  90%|████████▉ | 240/268 [02:36<00:18,  1.53it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  90%|████████▉ | 241/268 [02:36<00:17,  1.54it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  90%|█████████ | 242/268 [02:37<00:16,  1.54it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  91%|█████████ | 243/268 [02:37<00:16,  1.54it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  91%|█████████ | 244/268 [02:37<00:15,  1.55it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  91%|█████████▏| 245/268 [02:37<00:14,  1.55it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  92%|█████████▏| 246/268 [02:38<00:14,  1.56it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  92%|█████████▏| 247/268 [02:38<00:13,  1.56it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  93%|█████████▎| 248/268 [02:38<00:12,  1.56it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  93%|█████████▎| 249/268 [02:38<00:12,  1.57it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  93%|█████████▎| 250/268 [02:38<00:11,  1.57it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  94%|█████████▎| 251/268 [02:39<00:10,  1.58it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  94%|█████████▍| 252/268 [02:39<00:10,  1.58it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  94%|█████████▍| 253/268 [02:39<00:09,  1.58it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  95%|█████████▍| 254/268 [02:39<00:08,  1.59it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  95%|█████████▌| 255/268 [02:40<00:08,  1.59it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  96%|█████████▌| 256/268 [02:40<00:07,  1.60it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  96%|█████████▌| 257/268 [02:40<00:06,  1.60it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  96%|█████████▋| 258/268 [02:40<00:06,  1.60it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  97%|█████████▋| 259/268 [02:41<00:05,  1.61it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  97%|█████████▋| 260/268 [02:41<00:04,  1.61it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  97%|█████████▋| 261/268 [02:41<00:04,  1.62it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  98%|█████████▊| 262/268 [02:41<00:03,  1.62it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  98%|█████████▊| 263/268 [02:41<00:03,  1.62it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  99%|█████████▊| 264/268 [02:42<00:02,  1.63it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  99%|█████████▉| 265/268 [02:42<00:01,  1.63it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6:  99%|█████████▉| 266/268 [02:42<00:01,  1.64it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6: 100%|█████████▉| 267/268 [02:42<00:00,  1.64it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.023]\n",
      "Epoch 6: 100%|██████████| 268/268 [02:43<00:00,  1.64it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.0187]\n",
      "Epoch 6: 100%|██████████| 268/268 [02:43<00:00,  1.63it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.0187]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 1498: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 268/268 [02:44<00:00,  1.63it/s, loss=0.000856, v_num=3, train_loss=0.000804, val_loss=0.0187]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoints/bert-model-epoch=04-val_loss=0.02.ckpt', map_location='cpu')\n",
    "model.load_state_dict(state_dict['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_271953/2802461268.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  res = softmax(out).detach().cpu().numpy()\n"
     ]
    }
   ],
   "source": [
    "loader = data_module.val_dataloader()\n",
    "softmax = torch.nn.Softmax()\n",
    "\n",
    "predicts = []\n",
    "true = []\n",
    "\n",
    "for batch in loader:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"]\n",
    "    with torch.no_grad():\n",
    "        loss, out = model.forward(input_ids, attention_mask)\n",
    "    res = softmax(out).detach().cpu().numpy()\n",
    "    predicts.extend(res.tolist())\n",
    "    true.extend(labels.detach().cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix_plot(true, predicts, enc.categories_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_bool = np.round(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    recall: 0.9969\n",
      "    \n",
      "\n",
      "    precision: 0.9969\n",
      "    \n",
      "\n",
      "    roc auc macro: 0.9999,\n",
      "    roc auc micro: 0.9999\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "calculate_recall(true, preds_bool)\n",
    "calculate_precision(true, preds_bool)\n",
    "calculate_roc_auc(true, predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polar polar pulsuhr v800 hr  black  90060770 sports 29000multisports 29170heartratewatches sportscomputers gpsde\n",
      "true_cat: WASHINGMACHINES predicted: USB MEMORY\n",
      "\n",
      "None proph md wave aktion 0 63\n",
      "true_cat: BICYCLES predicted: CONTACT LENSES\n",
      "\n",
      "None kingston data traveler se9 mit 109\n",
      "true_cat: USB MEMORY predicted: BICYCLES\n",
      "\n",
      "candy candy evot 1005 1d weiss 6\n",
      "true_cat: WASHINGMACHINES predicted: BICYCLES\n",
      "\n",
      "candy gs 1482 d3 32516 washing machines\n",
      "true_cat: WASHINGMACHINES predicted: BICYCLES\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_cat = np.argmax(predicts, axis=1)\n",
    "true_cat = np.argmax(true, axis=1)\n",
    "\n",
    "wrong_indexes = np.where(predicted_cat != true_cat)[0]\n",
    "ind_to_category = {ind: cat for ind, cat in enumerate(enc.categories_[0])}\n",
    "\n",
    "for i in wrong_indexes:\n",
    "    print(test_data['concat_text'].iloc[i])\n",
    "    print(f'true_cat: {ind_to_category[true_cat[i]]}', f'predicted: {ind_to_category[predicted_cat[i]]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}